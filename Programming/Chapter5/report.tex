\documentclass[a4paper]{article}
\usepackage[affil-it]{authblk}
\usepackage{listings}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\geometry{margin=1.5cm, vmargin={0pt,1cm}}
\setlength{\topmargin}{-1cm}
\setlength{\paperheight}{29.7cm}
\setlength{\textheight}{25.3cm}

\begin{document}

\title{Numerical Analysis Programming Homework 5}
\author{DAVE JOVAN TANDIONO 3210300364 \thanks{Electronic address: minmindjt@gmail.com}}
\affil{Information and Mathematical Science 2101, Zhejiang University}
\date{Due time: October 29, 2024}

\maketitle

\begin{abstract}
This report covers the application of discrete least squares and QR factorization methods for polynomial fitting, highlighting numerical stability through condition numbers. Solutions analyze interpolation problems, illustrating coefficient structure and the impact of regularization on ill-conditioned systems.
\end{abstract}



\section*{Problem A}

The discrete least squares method was applied to determine the best-fit quadratic polynomial \( \hat{\varphi}(x) = a_0 + a_1 x + a_2 x^2 \) for the given data points \((x_i, y_i)\). The least squares solution was obtained using the normal equations:
\[
G \mathbf{a} = \mathbf{c},
\]
where \( G \) is the Gram matrix, \( \mathbf{a} \) is the vector of coefficients, and \( \mathbf{c} \) is the moment vector. The weight function used was \( \rho = 1 \).

The coefficients \( a_0, a_1, a_2 \) were computed using Gaussian elimination, yielding the best-fit quadratic polynomial:
\[
\hat{\varphi}(x) = -0.2384 x^2 + 2.6704 x + 2.1757.
\]

Figure~\ref{fig:least_squares} illustrates the fitted polynomial and the given data points. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{A.png}
    \caption{Discrete Least Squares Fit for the Given Data Points.}
    \label{fig:least_squares}
\end{figure}

\section*{Problem B}

The discrete least squares method was applied to determine the best-fit quadratic polynomial \( \hat{\varphi}(x) = a_0 + a_1 x + a_2 x^2 \) for the given data points \((x_i, y_i)\). The solution was obtained using QR factorization of the Vandermonde matrix \( A \), where each row corresponds to the powers of a data point \( x_i \) up to degree 2:
\[
A = 
\begin{bmatrix}
1 & x_1 & x_1^2 \\
1 & x_2 & x_2^2 \\
\vdots & \vdots & \vdots \\
1 & x_N & x_N^2
\end{bmatrix}.
\]

The factorization \( A = QR \) was computed, where \( Q \) is orthogonal and \( R \) is upper triangular. The system \( R \mathbf{a} = Q^T \mathbf{b} \) was then solved to find the coefficients \( \mathbf{a} = [a_0, a_1, a_2]^T \). The resulting polynomial is:
\[
\hat{\varphi}(x) = -0.2384 x^2 + 2.6704 x + 2.1757.
\]

To analyze the stability of the QR approach compared to the normal equations, the condition numbers of the matrices \( G \) and \( R_1 \) were calculated based on the 2-norm:
\[
\text{cond}(G) = 18556.6520, \quad \text{cond}(R) = 106.4415.
\]
The significantly smaller condition number for \( R \) confirms that the QR factorization approach is numerically more stable than the normal equation approach.

The two different algorithms yielded consistent coefficients, confirming their correctness:
\[
a_0 = 2.1757, \quad a_1 = 2.6704, \quad a_2 = -0.2384.
\]
This agreement further supports the accuracy of the QR factorization method.

\section*{Problem C}

To prove the structure of the solution to (5.40), we start by considering the right-hand side vector \( r_j \), defined as:
\[
r_j = \int_0^1 \frac{x^j}{1+x} \, \mathrm{d}x.
\]
This can be rewritten using the geometric series expansion:
\[
\frac{1}{1+x} = \sum_{i=0}^\infty (-x)^i,
\]
leading to:
\[
r_j = \int_0^1 x^j \left( \sum_{i=0}^\infty (-x)^i \right) \, \mathrm{d}x.
\]

Truncating the series at \( j \), we get:
\[
r_j = \int_0^1 \left( \frac{1}{1+x} - \sum_{i=1}^j (-x)^{i-1} \right) \, \mathrm{d}x.
\]

Using the closed form for the integral of \( \frac{1}{1+x} \) and the alternating harmonic series, this simplifies to:
\[
r_j = \ln 2 - \sum_{i=1}^j (-1)^{i-1} \frac{1}{i}.
\]

Thus, we can write:
\[
r_j = (-1)^j \ln 2 + \sum_{i=1}^j (-1)^{i+j} \frac{1}{i}.
\]

The normal equations (5.40) are given by:
\[
A \alpha = r,
\]
where \( A \) is the \((n+1) \times (n+1)\) Hilbert matrix with elements \( A_{jk} = \frac{1}{j+k+1} \), \( \alpha \) is the unknown coefficient vector, and \( r \) is the vector with entries \( r_j \).

From the expression for \( r_j \), we observe that:
\[
r_j = \beta_j \ln 2 + \gamma_j,
\]
where:
\[
\beta_j = (-1)^j, \quad \gamma_j = \sum_{i=1}^j (-1)^{i+j} \frac{1}{i}.
\]

Both \( \beta_j \) and \( \gamma_j \) are composed of rational numbers:
- \( \beta_j = (-1)^j \), clearly rational.
- \( \gamma_j = \sum_{i=1}^j (-1)^{i+j} \frac{1}{i} \), as the harmonic numbers are rational sums.

Additionally, since every element of \( A^{-1} \) is rational (as the inverse of a Hilbert matrix has a known formula involving factorials, which are products of integers), the coefficients \( \alpha_j \) can be expressed as:
\[
\alpha_j = \beta_j \ln 2 + \gamma_j,
\]
where \( \beta_j \) and \( \gamma_j \) are rational numbers for all \( j = 0, 1, \ldots, n \).

To verify the theoretical results, a numerical solution was computed for \( n = 1, 2, \ldots, 6 \), using the following steps:
1. Solving \( A \alpha = r \) for \( \alpha \), where \( r_j \) is computed with \( \ln 2 \) approximated to machine precision (\( \approx 0.693147 \)).
2. Computing \( \alpha_j \) using floating-point arithmetic with \( \ln 2 \approx 0.69315 \) (relative error \( 10^{-5} \)).
3. Analyzing the effect of the condition number of the Hilbert matrix for \( n = 2, 3, \ldots, 6 \) on the solution.
4. Applying Tikhonov regularization (\( \alpha = 10^{-10} \)) to improve convergence.

The condition number of the Hilbert matrix grows rapidly with \( n \):
\[
\text{cond}(H) = 
\begin{cases}
1.0 & n = 1, \\
19.28 & n = 2, \\
524.06 & n = 3, \\
15513.74 & n = 4, \\
476607.25 & n = 5, \\
1.495 \times 10^7 & n = 6.
\end{cases}
\]

When \( \ln 2 \) is approximated with a relative error of \( 10^{-5} \), convergence deteriorates due to the high condition numbers of the Hilbert matrix. However, applying Tikhonov regularization with \( \alpha = 10^{-10} \) restores convergence for the first few coefficients (\( \alpha_0 \) and \( \alpha_1 \)), demonstrating the stabilizing effect of regularization.

The numerical results confirm the theoretical prediction that \( \alpha_j = \beta_j \ln 2 + \gamma_j \), with \( \beta_j \) and \( \gamma_j \) being rational numbers. Despite the ill-conditioning of the Hilbert matrix for larger \( n \), Tikhonov regularization mitigates the instability, allowing accurate recovery of low-order coefficients.

\section*{\center{\normalsize {Acknowledgment}}}
I would like to acknowledge the assistance of ChatGPT, which provided valuable insights and support in understanding the concepts, structuring the solutions, and developing the code for this assignment. 

\end{document}


